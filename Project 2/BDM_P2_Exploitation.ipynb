{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yutao-data/Apache_Hive_TPC_DI/blob/master/Project%202/BDM_P2_Exploitation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtl59G6OS8Ml",
        "outputId": "5eee007d-c6da-414b-bf0b-62dc43c8cd5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=c4fa3fa353d306e3534e9178a4ba99b84b95fd7a17a78eb8153aef1222336edb\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting delta-spark\n",
            "  Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (3.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark) (7.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.19.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark<3.6.0,>=3.5.0->delta-spark) (0.10.9.7)\n",
            "Installing collected packages: delta-spark\n",
            "Successfully installed delta-spark-3.2.0\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.14.0-py3-none-any.whl (25.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.6)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.25.2)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow)\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow)\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow) (24.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.3)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (14.0.2)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Collecting querystring-parser<2 (from mlflow)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.11.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.30)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.5.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Collecting gunicorn<23 (from mlflow)\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.0.7)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow) (1.14.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: aniso8601, smmap, querystring-parser, Mako, gunicorn, graphql-core, deprecated, opentelemetry-api, graphql-relay, gitdb, docker, alembic, opentelemetry-semantic-conventions, graphene, gitpython, opentelemetry-sdk, mlflow\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 aniso8601-9.0.1 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 mlflow-2.14.0 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 querystring-parser-1.2.4 smmap-5.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install delta-spark\n",
        "!pip install statsmodels\n",
        "!pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VSlth008TJDF"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.spark\n",
        "from google.colab import drive\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "from pyspark.mllib.feature import StandardScaler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, avg, count, when, datediff, current_date, to_date, regexp_replace, month, dayofmonth, sum as _sum, coalesce, lit, isnan, unix_timestamp, lag, from_json, lead, rand, percentile_approx\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, LongType, StringType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import Row\n",
        "from delta.tables import DeltaTable\n",
        "from delta import configure_spark_with_delta_pip\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7K0Q66qdxrv",
        "outputId": "426207a6-d1c1-4388-d544-cb8cfe07e7e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCAKDf199cFO",
        "outputId": "3e7f164d-9acb-4241-f3a8-eccf8e2ad39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta Lake loaded: True\n"
          ]
        }
      ],
      "source": [
        "# Initialize a Spark session\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"DataProcessingWithDeltaLake\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "delta_extensions = spark.conf.get(\"spark.sql.extensions\", \"\")\n",
        "print(\"Delta Lake loaded:\", \"io.delta.sql.DeltaSparkSessionExtension\" in delta_extensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUgmPfPOA3g2"
      },
      "source": [
        "# Exploitation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yOn-WmXA8aL"
      },
      "source": [
        "## Descriptive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a03K0oA4ktsP"
      },
      "outputs": [],
      "source": [
        "delta_lake_path = \"/content/drive/MyDrive/BDM/Data/Formatted_Zone/filtered_df\"\n",
        "\n",
        "# Read the Delta Lake table\n",
        "filtered_df = spark.read.format(\"delta\").load(delta_lake_path)\n",
        "filtered_df.printSchema()\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlLO14scITaY"
      },
      "source": [
        "### KPIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akW8-2HXBBMK"
      },
      "outputs": [],
      "source": [
        "# Average price per area\n",
        "avg_price_per_neighbourhood_group = filtered_df.groupBy(\"neighbourhood_group\").agg(F.avg(\"price\").alias(\"average_price\"))\n",
        "avg_price_per_neighbourhood_group.show()\n",
        "\n",
        "# Average price per room type\n",
        "avg_price_per_room_type = filtered_df.groupBy(\"room_type\").agg(F.avg(\"price\").alias(\"average_price\"))\n",
        "avg_price_per_room_type.show()\n",
        "\n",
        "# Vacancy rate per area\n",
        "availability_rate_per_neighbourhood_group = filtered_df.groupBy(\"neighbourhood_group\").agg(F.avg(\"availability_percentage\").alias(\"availability_rate\"))\n",
        "availability_rate_per_neighbourhood_group.show()\n",
        "\n",
        "# Trends in the total number of tourists per month\n",
        "monthly_tourists_trend = filtered_df.groupBy(F.month(\"date\").alias(\"month\")).agg(F.sum(\"Total_Tourists\").alias(\"total_tourists\"))\n",
        "monthly_tourists_trend.show()\n",
        "\n",
        "# Save KPIs\n",
        "avg_price_per_neighbourhood_group.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/avg_price_per_neighbourhood_group.csv\", header=True, mode=\"overwrite\")\n",
        "avg_price_per_room_type.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/avg_price_per_room_type.csv\", header=True, mode=\"overwrite\")\n",
        "availability_rate_per_neighbourhood_group.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/availability_rate_per_neighbourhood_group.csv\", header=True, mode=\"overwrite\")\n",
        "monthly_tourists_trend.coalesce(1).write.csv(\"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/KPIs/monthly_tourists_trend.csv\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huwSV91OIVV1"
      },
      "source": [
        "## Predictive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_lake_path = \"/content/drive/MyDrive/BDM/Data/Formatted_Zone/filtered_df\"\n",
        "\n",
        "# Read the Delta Lake table\n",
        "filtered_df = spark.read.format(\"delta\").load(delta_lake_path)"
      ],
      "metadata": {
        "id": "beEnfDt9m1pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxWSAc2gIcH7"
      },
      "source": [
        "### Data Splict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTI1nKL_XCZC"
      },
      "outputs": [],
      "source": [
        "# spark = SparkSession.builder.appName(\"TouristRegression\").getOrCreate()\n",
        "# sc = spark.sparkContext\n",
        "\n",
        "# df = filtered_df.select(\"Germany_Tourists\", \"Italy_Tourists\").limit(5000)\n",
        "# df.show(5)\n",
        "\n",
        "# rdd = df.rdd.map(lambda row: LabeledPoint(row.Italy_Tourists, [row.Germany_Tourists]))\n",
        "# rdd = rdd.sample(False, 1.0, seed=42)\n",
        "\n",
        "# features = np.array([point.features[0] for point in rdd.collect()])\n",
        "# labels = np.array([point.label for point in rdd.collect()])\n",
        "\n",
        "# mean = np.mean(features)\n",
        "# std = np.std(features)\n",
        "# normalized_features = (features - mean) / std\n",
        "\n",
        "# normalized_data = [LabeledPoint(label, [feature]) for label, feature in zip(labels, normalized_features)]\n",
        "\n",
        "# train_rdd, test_rdd = sc.parallelize(normalized_data).randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"TouristRegression\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "df = filtered_df.select(\"Germany_Tourists\").limit(5000)\n",
        "df.show(5)\n",
        "\n",
        "tourist_data = df.collect()\n",
        "labeled_points = []\n",
        "for i in range(len(tourist_data) - 1):\n",
        "    current_day = tourist_data[i]['Germany_Tourists']\n",
        "    next_day = tourist_data[i + 1]['Germany_Tourists']\n",
        "    labeled_points.append(LabeledPoint(next_day, [current_day]))\n",
        "\n",
        "rdd = sc.parallelize(labeled_points)\n",
        "\n",
        "features = np.array([point.features[0] for point in rdd.collect()])\n",
        "labels = np.array([point.label for point in rdd.collect()])\n",
        "\n",
        "mean = np.mean(features)\n",
        "std = np.std(features)\n",
        "normalized_features = (features - mean) / std\n",
        "\n",
        "normalized_data = [LabeledPoint(label, [feature]) for label, feature in zip(labels, normalized_features)]\n",
        "\n",
        "train_rdd, test_rdd = sc.parallelize(normalized_data).randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "VEDpN4kspaiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1eD8Il_IeHf"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1oVs-80IX61"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "iterations = 100\n",
        "\n",
        "lrm = LinearRegressionWithSGD.train(train_rdd, iterations=iterations, step=learning_rate, initialWeights=np.array([1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENZCXmLhVQWW"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = np.array([point.features[0] for point in test_rdd.collect()])\n",
        "test_labels = np.array(test_rdd.map(lambda p: p.label).collect())\n",
        "test_original = test_features * std + mean\n",
        "\n",
        "predictions = []\n",
        "for feature in test_original:\n",
        "    prediction = lrm.predict([feature])\n",
        "    predictions.append(prediction)\n",
        "\n",
        "mse = mean_squared_error(test_labels, predictions)\n",
        "mae = mean_absolute_error(test_labels, predictions)\n",
        "r2 = r2_score(test_labels, predictions)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R²): {r2}\")"
      ],
      "metadata": {
        "id": "p-FPCsYtgj5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_utg7VjVSbF"
      },
      "source": [
        "### Model & Predictions Store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = spark.createDataFrame([Row(label=float(l), prediction=float(p)) for l, p in zip(test_labels, predictions)])"
      ],
      "metadata": {
        "id": "mgkLpEqzh1wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HSRfI-MWMft"
      },
      "outputs": [],
      "source": [
        "output_path = \"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/predictions.csv\"\n",
        "results.write.mode(\"overwrite\").csv(output_path, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RizSRfJp4i-u"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model\"\n",
        "if os.path.exists(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "\n",
        "lrm.save(sc, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pULKgmnnIne-"
      },
      "source": [
        "# Stream analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkmLPHLLdk5g"
      },
      "source": [
        "## Data Flow Simulator Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGOm5npSIodm"
      },
      "outputs": [],
      "source": [
        "sample_df = filtered_df.limit(10).toPandas()\n",
        "\n",
        "sample_df['date'] = sample_df['date'].astype(str)\n",
        "sample_df['last_review'] = sample_df['last_review'].astype(str)\n",
        "\n",
        "data_dir = \"/content/data_stream\"\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcqTxlHidnBx"
      },
      "source": [
        "## Spark Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKK5UyCxebgq"
      },
      "outputs": [],
      "source": [
        "# for index, row in sample_df.iterrows():\n",
        "#     message = row.to_dict()\n",
        "#     file_path = os.path.join(data_dir, f\"data_{index}.json\")\n",
        "#     with open(file_path, \"w\") as file:\n",
        "#         json.dump(message, file)\n",
        "#     time.sleep(1)\n",
        "\n",
        "# schema = StructType([\n",
        "#     StructField(\"id\", LongType(), True),\n",
        "#     StructField(\"host_id\", IntegerType(), True),\n",
        "#     StructField(\"latitude\", DoubleType(), True),\n",
        "#     StructField(\"longitude\", DoubleType(), True),\n",
        "#     StructField(\"number_of_reviews\", IntegerType(), True),\n",
        "#     StructField(\"last_review\", StringType(), True),\n",
        "#     StructField(\"reviews_per_month\", DoubleType(), True),\n",
        "#     StructField(\"calculated_host_listings_count\", IntegerType(), True),\n",
        "#     StructField(\"availability_365\", IntegerType(), True),\n",
        "#     StructField(\"number_of_reviews_ltm\", IntegerType(), True),\n",
        "#     StructField(\"days_since_last_review\", IntegerType(), True),\n",
        "#     StructField(\"room_type_encoded\", IntegerType(), True),\n",
        "#     StructField(\"date\", StringType(), True),\n",
        "#     StructField(\"price\", DoubleType(), True),\n",
        "#     StructField(\"minimum_nights\", IntegerType(), True),\n",
        "#     StructField(\"maximum_nights\", IntegerType(), True),\n",
        "#     StructField(\"availability_percentage\", DoubleType(), True),\n",
        "#     StructField(\"Germany_Tourists\", LongType(), True),\n",
        "#     StructField(\"Spain_Tourists\", LongType(), True),\n",
        "#     StructField(\"France_Tourists\", LongType(), True),\n",
        "#     StructField(\"Italy_Tourists\", LongType(), True),\n",
        "#     StructField(\"UK_Tourists\", LongType(), True),\n",
        "#     StructField(\"Total_Tourists\", LongType(), True),\n",
        "# ])\n",
        "\n",
        "# model_path = \"/content/drive/MyDrive/BDM/Data/Exploitation_Zone/lr_model\"\n",
        "# lr_model = LinearRegressionWithSGD.load(model_path)\n",
        "\n",
        "# df_stream = spark \\\n",
        "#     .readStream \\\n",
        "#     .schema(schema) \\\n",
        "#     .json(\"/content/data_stream\")\n",
        "\n",
        "# feature_columns = [\n",
        "#     \"host_id\", \"latitude\", \"longitude\", \"room_type_encoded\", \"number_of_reviews\",\n",
        "#     \"reviews_per_month\", \"calculated_host_listings_count\", \"availability_365\",\n",
        "#     \"days_since_last_review\", \"price\", \"minimum_nights\", \"maximum_nights\",\n",
        "#     \"availability_percentage\", \"Germany_Tourists\", \"Spain_Tourists\", \"France_Tourists\",\n",
        "#     \"Italy_Tourists\", \"UK_Tourists\", \"Total_Tourists\"\n",
        "# ]\n",
        "\n",
        "# assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "# df_features = assembler.transform(df_stream)\n",
        "\n",
        "# predictions = lr_model.transform(df_features)\n",
        "\n",
        "# output_path = \"/content/predictions_output\"\n",
        "\n",
        "# query = predictions.select(\"features\", \"prediction\", \"Total_Tourists\", \"date\").writeStream \\\n",
        "#     .outputMode(\"append\") \\\n",
        "#     .format(\"json\") \\\n",
        "#     .option(\"path\", output_path) \\\n",
        "#     .option(\"checkpointLocation\", \"/content/checkpoint\") \\\n",
        "#     .start()\n",
        "\n",
        "# time.sleep(20)\n",
        "# query.stop()\n",
        "\n",
        "# result_files = glob.glob(f\"{output_path}/*.json\")\n",
        "# results = []\n",
        "\n",
        "# for file in result_files:\n",
        "#     with open(file, \"r\") as f:\n",
        "#         for line in f:\n",
        "#             results.append(json.loads(line))\n",
        "\n",
        "# results_df = pd.DataFrame(results)\n",
        "# print(results_df.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}